{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Word into Vectors\n",
    "- Article that generates the idea: [Neural Word Embedding as Implicit Matrix Factorization](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)\n",
    "- Text8 used by google's word2vec [text8 data](http://mattmahoney.net/dc/text8.zip)\n",
    "- [wordsim353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
    "- [MEN Test collection](http://clic.cimec.unitn.it/~elia.bruni/MEN)\n",
    "\n",
    "## Experimental Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.85 s, sys: 307 ms, total: 3.16 s\n",
      "Wall time: 3.16 s\n",
      "17005207\n"
     ]
    }
   ],
   "source": [
    "## load text data\n",
    "import re\n",
    "%time corpus = re.findall(r\"\\w+\", open(\"data/text8\").read())\n",
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils import check_array, as_float_array, check_random_state\n",
    "from sklearn.utils.extmath import randomized_svd, safe_sparse_dot, svd_flip\n",
    "from sklearn.utils.sparsefuncs import mean_variance_axis\n",
    "\n",
    "import scipy.sparse as sp\n",
    "class SymmetricSVD(TruncatedSVD):\n",
    "    def __init__(self, n_components=2, algorithm=\"randomized\", n_iter=5,\n",
    "                    random_state=None, tol=0.):\n",
    "        super(SymmetricSVD, self).__init__(n_components, algorithm, \n",
    "                                          n_iter, random_state, tol)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        After svd, we have M = U * Sigma * VT, traditional SVD\n",
    "        return W = U * Sigma as the transformed vectors; here \n",
    "        in SymmetricSVD version, it is W = U * sqrt(Sigma) that is returned\n",
    "        as the transformed vectors.\n",
    "        In the paper [Neural Word Embedding as Implicit Matrix Factorization], the \n",
    "        authors claim that it is not clear why this works better, but it does work\n",
    "        better than the traditional approach.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Reduced version of X. This will always be a dense array.\n",
    "        \"\"\"\n",
    "        X = as_float_array(X, copy=False)\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        # If sparse and not csr or csc, convert to csr\n",
    "        if sp.issparse(X) and X.getformat() not in [\"csr\", \"csc\"]:\n",
    "            X = X.tocsr()\n",
    "\n",
    "        if self.algorithm == \"arpack\":\n",
    "            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "            # conventions, so reverse its outputs.\n",
    "            Sigma = Sigma[::-1]\n",
    "            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "        elif self.algorithm == \"randomized\":\n",
    "            k = self.n_components\n",
    "            n_features = X.shape[1]\n",
    "            if k >= n_features:\n",
    "                raise ValueError(\"n_components must be < n_features;\"\n",
    "                                 \" got %d >= %d\" % (k, n_features))\n",
    "            U, Sigma, VT = randomized_svd(X, self.n_components,\n",
    "                                          n_iter=self.n_iter,\n",
    "                                          random_state=random_state)\n",
    "        else:\n",
    "            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n",
    "\n",
    "        self.components_ = VT\n",
    "\n",
    "        # Calculate explained variance & explained variance ratio\n",
    "        ## USE SQRT OF SIGMA INSTEAD OF SIGMA ITSELF\n",
    "        X_transformed = np.dot(U, np.sqrt(np.diag(Sigma)))\n",
    "        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n",
    "        if sp.issparse(X):\n",
    "            _, full_var = mean_variance_axis(X, axis=0)\n",
    "            full_var = full_var.sum()\n",
    "        else:\n",
    "            full_var = np.var(X, axis=0).sum()\n",
    "        self.explained_variance_ratio_ = exp_var / full_var\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "class MFWordEmbedder(object):\n",
    "    \"\"\"Matrix Factorization Word Embedder\n",
    "    \"\"\"\n",
    "    def __init__(self, min_wf = 120, min_cf = 6, window = 2, nneg = 1., \n",
    "                 svdtype=\"truncated\", vec_dim = 100):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        min_wf - minimum number of frequence for a word to be modelled\n",
    "        min_cf - minimum number of frequence for a context (word window) to be modelled\n",
    "        window - int or list, length(s) for word seq for context\n",
    "        nneg - float number of negative sample, plays as the offset in the matrix factorization here\n",
    "        svdtype - string {\"symmetric\", \"truncated\"}\n",
    "        vec_dim - int: dimension of learned word vectors\n",
    "        \"\"\"\n",
    "        self.min_wf = min_wf\n",
    "        self.min_cf = min_cf\n",
    "        self.window = window\n",
    "        self.nneg = nneg\n",
    "        self.svdtype = svdtype\n",
    "        self.vec_dim = vec_dim\n",
    "    \n",
    "    def build_hash(self, words):\n",
    "        windows = [self.window] if isinstance(self.window, int) else sorted(self.window)\n",
    "        word_hash, nwords = {}, 0\n",
    "        context_hash, ncontexts = {}, 0\n",
    "        # row/col for matrix - only words >= min_wf and contexts >= min_cf will get in\n",
    "        wordhash2row, nrows = {}, 0 \n",
    "        contexthash2col, ncols = {}, 0\n",
    "        ## (wordhash, contexthash) pairs\n",
    "        wh_ch_pairs = []\n",
    "        \n",
    "        for iw, window in enumerate(windows):\n",
    "            for i in xrange(window, len(words)-window):\n",
    "                word, context = words[i], tuple(words[i-window:i]+words[i+1:i+window+1])\n",
    "                ## only update word hash for the first window scanning\n",
    "                if (iw == 0): \n",
    "                    if word in word_hash:\n",
    "                        h, n = word_hash[word]\n",
    "                        word_hash[word] = (h, n+1)\n",
    "                    else:\n",
    "                        word_hash[word] = (nwords, 1)\n",
    "                        nwords += 1\n",
    "                    ## update rows if word occure frequent enought\n",
    "                    if word_hash[word][1] == self.min_wf: \n",
    "                        wordhash2row[word_hash[word][0]] = nrows\n",
    "                        nrows += 1\n",
    "                ## update context hash\n",
    "                if context in context_hash:\n",
    "                    h, n = context_hash[context]\n",
    "                    context_hash[context] = (h, n+1)\n",
    "                else:\n",
    "                    context_hash[context] = (ncontexts, 1)\n",
    "                    ncontexts += 1\n",
    "                ## update cols if context occure frequently enough\n",
    "                if context_hash[context][1] == self.min_cf:\n",
    "                    contexthash2col[context_hash[context][0]] = ncols\n",
    "                    ncols += 1\n",
    "                ## update wordhash, contexthash pairs\n",
    "                wh_ch_pairs.append( (word_hash[word][0], context_hash[context][0]) )\n",
    "#         wh_ch_pairs = np.array([(wid, cid) for wid, cid in wh_ch_pairs if wid in wordhash2row\n",
    "#                                                                     if cid in contexthash2col])\n",
    "        ## row to word ????\n",
    "        return word_hash, context_hash, wordhash2row, contexthash2col, wh_ch_pairs\n",
    "    \n",
    "    def build_matrix(self, wordhash2row, contexthash2col, wh_ch_pairs):\n",
    "        wordids, contextids = zip(*wh_ch_pairs)\n",
    "        widcounter = Counter(wordids)\n",
    "        cidcounter = Counter(contextids)\n",
    "        npairs = len(wh_ch_pairs) * 1.\n",
    "        nrows, ncols = len(wordhash2row), len(contexthash2col)\n",
    "        \n",
    "#         data = np.array([npairs / widcounter[wid] / cidcounter[cid] for wid, cid in wh_ch_pairs])\n",
    "#         rows = np.array([wordhash2row[wid] for wid in wordids])\n",
    "#         cols = np.array([contexthash2col[cid] for cid in contextids])\n",
    "\n",
    "        data, rows, cols = [], [], []\n",
    "        for wid, cid in wh_ch_pairs:\n",
    "            if wid in wordhash2row and cid in contexthash2col:\n",
    "                data.append( npairs / widcounter[wid] / cidcounter[cid] )\n",
    "                rows.append(wordhash2row[wid])\n",
    "                cols.append(contexthash2col[cid])\n",
    "        data, rows, cols = np.array(data), np.array(rows), np.array(cols)\n",
    "        M = sparse.coo_matrix( (data, (rows, cols)), shape = (nrows, ncols), dtype=np.float32 )\n",
    "        M.data = np.log(M.data)\n",
    "        M = M.tocsr()\n",
    "        M[M<0.0] = 0.0\n",
    "        return M\n",
    "    \n",
    "    def svd(self, M, *args, **kwargs):\n",
    "        svd = (TruncatedSVD(n_components=self.vec_dim, *args, **kwargs) \n",
    "               if self.svdtype == \"truncated\" \n",
    "               else SymmetricSVD(n_components=self.vec_dim, *args, **kwargs))\n",
    "        self.word_vectors = svd.fit_transform(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "model = MFWordEmbedder(min_wf=2, min_cf=1, window=2, vec_dim=2)\n",
    "a = corpus[:10] * 2\n",
    "print a\n",
    "word_hash, context_hash, wordhash2row, contexthash2col, wh_ch_pairs = model.build_hash(a)\n",
    "M = model.build_matrix(wordhash2row, contexthash2col, wh_ch_pairs)\n",
    "model.svd(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (1, 2), 'term': (2, 2), 'used': (6, 1), 'anarchism': (8, 1), 'originated': (9, 1), 'of': (3, 2), 'against': (7, 1), 'as': (0, 2), 'abuse': (4, 2), 'first': (5, 2)}\n",
      "{('a', 'term', 'abuse', 'first'): (3, 2), ('term', 'of', 'first', 'used'): (4, 2), ('originated', 'as', 'term', 'of'): (1, 2), ('anarchism', 'originated', 'a', 'term'): (0, 2), ('abuse', 'first', 'against', 'anarchism'): (6, 1), ('as', 'a', 'of', 'abuse'): (2, 2), ('of', 'abuse', 'used', 'against'): (5, 2), ('against', 'anarchism', 'as', 'a'): (9, 1), ('first', 'used', 'anarchism', 'originated'): (7, 1), ('used', 'against', 'originated', 'as'): (8, 1)}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5)]\n"
     ]
    }
   ],
   "source": [
    "print word_hash\n",
    "print context_hash\n",
    "print wordhash2row\n",
    "print contexthash2col\n",
    "print wh_ch_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.4 s, sys: 1.47 s, total: 31.8 s\n",
      "Wall time: 31.8 s\n",
      "CPU times: user 31.3 s, sys: 1.11 s, total: 32.4 s\n",
      "Wall time: 32.5 s\n",
      "CPU times: user 15.1 s, sys: 7.06 s, total: 22.1 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "model = MFWordEmbedder()\n",
    "a = corpus\n",
    "%time word_hash, context_hash, wordhash2row, contexthash2col, wh_ch_pairs = model.build_hash(a)\n",
    "%time M = model.build_matrix(wordhash2row, contexthash2col, wh_ch_pairs)\n",
    "%time model.svd(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
