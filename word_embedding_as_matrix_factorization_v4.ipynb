{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Word into Vectors\n",
    "- Article that generates the idea: [Neural Word Embedding as Implicit Matrix Factorization](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)\n",
    "- Text8 used by google's word2vec [text8 data](http://mattmahoney.net/dc/text8.zip)\n",
    "- [wordsim353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
    "- [MEN Test collection](http://clic.cimec.unitn.it/~elia.bruni/MEN)\n",
    "\n",
    "## Algorithm Design Considerations\n",
    "- we extend the original algorithm from allowing a fixed length window to a set of different length windows - not sure whether it will improve the result\n",
    "- the algorithm filter out low freq words and contexts. Current impl may result in all-zero rows in the matrix when the row is a frequent word that is linked to a lot of infrequent contexts - this is very unlikely though.\n",
    "- we count the word frequency once for different windows, it will only affect the frequency filter for word\n",
    "\n",
    "## Experimental Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 253 ms, total: 2.9 s\n",
      "Wall time: 2.9 s\n",
      "17005207\n"
     ]
    }
   ],
   "source": [
    "## load text data\n",
    "import re\n",
    "corpus = open(\"data/text8\").read()\n",
    "#%time corpus = corpus.replace(\".\", \" DOT DOT\")\n",
    "%time corpus = re.findall(r\"\\w+\", corpus)\n",
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils import check_array, as_float_array, check_random_state\n",
    "from sklearn.utils.extmath import randomized_svd, safe_sparse_dot, svd_flip\n",
    "from sklearn.utils.sparsefuncs import mean_variance_axis\n",
    "\n",
    "import scipy.sparse as sp\n",
    "class SymmetricSVD(TruncatedSVD):\n",
    "    def __init__(self, n_components=2, algorithm=\"randomized\", n_iter=5,\n",
    "                    random_state=None, tol=0.):\n",
    "        super(SymmetricSVD, self).__init__(n_components, algorithm, \n",
    "                                          n_iter, random_state, tol)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        After svd, we have M = U * Sigma * VT, traditional SVD\n",
    "        return W = U * Sigma as the transformed vectors; here \n",
    "        in SymmetricSVD version, it is W = U * sqrt(Sigma) that is returned\n",
    "        as the transformed vectors.\n",
    "        In the paper [Neural Word Embedding as Implicit Matrix Factorization], the \n",
    "        authors claim that it is not clear why this works better, but it does work\n",
    "        better than the traditional approach.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Reduced version of X. This will always be a dense array.\n",
    "        \"\"\"\n",
    "        X = as_float_array(X, copy=False)\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        # If sparse and not csr or csc, convert to csr\n",
    "        if sp.issparse(X) and X.getformat() not in [\"csr\", \"csc\"]:\n",
    "            X = X.tocsr()\n",
    "\n",
    "        if self.algorithm == \"arpack\":\n",
    "            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "            # conventions, so reverse its outputs.\n",
    "            Sigma = Sigma[::-1]\n",
    "            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "        elif self.algorithm == \"randomized\":\n",
    "            k = self.n_components\n",
    "            n_features = X.shape[1]\n",
    "            if k >= n_features:\n",
    "                raise ValueError(\"n_components must be < n_features;\"\n",
    "                                 \" got %d >= %d\" % (k, n_features))\n",
    "            U, Sigma, VT = randomized_svd(X, self.n_components,\n",
    "                                          n_iter=self.n_iter,\n",
    "                                          random_state=random_state)\n",
    "        else:\n",
    "            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n",
    "\n",
    "        self.components_ = VT\n",
    "\n",
    "        # Calculate explained variance & explained variance ratio\n",
    "        ## USE SQRT OF SIGMA INSTEAD OF SIGMA ITSELF\n",
    "        X_transformed = np.dot(U, np.sqrt(np.diag(Sigma)))\n",
    "        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n",
    "        if sp.issparse(X):\n",
    "            _, full_var = mean_variance_axis(X, axis=0)\n",
    "            full_var = full_var.sum()\n",
    "        else:\n",
    "            full_var = np.var(X, axis=0).sum()\n",
    "        self.explained_variance_ratio_ = exp_var / full_var\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MFWordEmbedder(object):\n",
    "    \"\"\"Matrix Factorization Word Embedder\n",
    "    \"\"\"\n",
    "    def __init__(self, min_wf = 120, min_cf = 6, window = 2, nneg = 1., \n",
    "                 svdtype=\"truncated\", vec_dim = 100):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        min_wf - minimum number of frequence for a word to be modelled\n",
    "        min_cf - minimum number of frequence for a context (word window) to be modelled\n",
    "        window - int or list, length(s) for word seq for context\n",
    "        nneg - float number of negative sample, plays as the offset in the matrix factorization here\n",
    "        svdtype - string {\"symmetric\", \"truncated\"}\n",
    "        vec_dim - int: dimension of learned word vectors\n",
    "        \"\"\"\n",
    "        self.min_wf = min_wf\n",
    "        self.min_cf = min_cf\n",
    "        self.window = window\n",
    "        self.nneg = nneg\n",
    "        self.svdtype = svdtype\n",
    "        self.vec_dim = vec_dim\n",
    "    \n",
    "    def build_hash(self, words):\n",
    "        windows = [self.window] if isinstance(self.window, int) else sorted(self.window)\n",
    "        word_hash, nwords = {}, 0\n",
    "        context_hash, ncontexts = {}, 0\n",
    "        # row/col for matrix - only words >= min_wf and contexts >= min_cf will get in\n",
    "        wordhash2row, nrows = {}, 0 \n",
    "        contexthash2col, ncols = {}, 0\n",
    "        ## (wordhash, contexthash) pairs\n",
    "        wh_ch_pairs = []\n",
    "        \n",
    "        for iw, window in enumerate(windows):\n",
    "            for i in xrange(window, len(words)-window):\n",
    "                word, context = words[i], tuple(words[i-window:i]+words[i+1:i+window+1])\n",
    "                ## only update word hash for the first window scanning\n",
    "                if (iw == 0): \n",
    "                    if word in word_hash:\n",
    "                        h, n = word_hash[word]\n",
    "                        word_hash[word] = (h, n+1)\n",
    "                    else:\n",
    "                        word_hash[word] = (nwords, 1)\n",
    "                        nwords += 1\n",
    "                    ## update rows if word occure frequent enought\n",
    "                    if word_hash[word][1] == self.min_wf: \n",
    "                        wordhash2row[word_hash[word][0]] = nrows\n",
    "                        nrows += 1\n",
    "                ## update context hash\n",
    "                if context in context_hash:\n",
    "                    h, n = context_hash[context]\n",
    "                    context_hash[context] = (h, n+1)\n",
    "                else:\n",
    "                    context_hash[context] = (ncontexts, 1)\n",
    "                    ncontexts += 1\n",
    "                ## update cols if context occure frequently enough\n",
    "                if context_hash[context][1] == self.min_cf:\n",
    "                    contexthash2col[context_hash[context][0]] = ncols\n",
    "                    ncols += 1\n",
    "                ## update wordhash, contexthash pairs\n",
    "                wh_ch_pairs.append( (word_hash[word][0], context_hash[context][0]) )\n",
    "        \n",
    "        wh_ch_pairs = np.array([(wid, cid) for wid, cid in wh_ch_pairs \n",
    "                                           if wid in wordhash2row if cid in contexthash2col])\n",
    "        inv_word_hash = dict([(h,w) for w,(h,n) in word_hash.items()])\n",
    "        row2hash = dict([(r, h) for h, r in wordhash2row.items()])\n",
    "        self.words = np.array([inv_word_hash[row2hash[i]] for i in xrange(len(row2hash))])\n",
    "        #self.row2word = dict([(r, inv_word_hash[h]) for h, r in wordhash2row.items()])\n",
    "        #self.word2row = dict([(v,k) for k,v in self.row2word.items()])\n",
    "        return word_hash, context_hash, wordhash2row, contexthash2col, wh_ch_pairs\n",
    "    \n",
    "    def build_matrix(self, wordhash2row, contexthash2col, wh_ch_pairs):\n",
    "        wordids, contextids = zip(*wh_ch_pairs)\n",
    "        widcounter = Counter(wordids)\n",
    "        cidcounter = Counter(contextids)\n",
    "        npairs = len(wh_ch_pairs) * 1.\n",
    "        nrows, ncols = len(wordhash2row), len(contexthash2col)\n",
    "        \n",
    "        ## some rows in M might be all-zero if a frequent word is linked to a lot of infrequent \n",
    "        ## contexts, even though it is expected to be rare in practice\n",
    "        data = np.array([npairs / widcounter[wid] / cidcounter[cid] for wid, cid in wh_ch_pairs])\n",
    "        rows = np.array([wordhash2row[wid] for wid in wordids])\n",
    "        cols = np.array([contexthash2col[cid] for cid in contextids])\n",
    "        \n",
    "        M = sparse.coo_matrix( (data, (rows, cols)), shape = (nrows, ncols), dtype=np.float32 )\n",
    "        M.data = np.log(M.data) - np.log(self.nneg)\n",
    "        M = M.tocsr()\n",
    "        M[M<0.0] = 0.0\n",
    "        return M\n",
    "    \n",
    "    def svd(self, M, *args, **kwargs):\n",
    "        svd = (TruncatedSVD(n_components=self.vec_dim, *args, **kwargs) \n",
    "               if self.svdtype == \"truncated\" \n",
    "               else SymmetricSVD(n_components=self.vec_dim, *args, **kwargs))\n",
    "        word_vectors = svd.fit_transform(M)\n",
    "        return word_vectors\n",
    "    \n",
    "    def fit(self, words):\n",
    "        word_hash, context_hash, wordhash2row, contexthash2col, wh_ch_pairs = self.build_hash(words)\n",
    "        M = model.build_matrix(wordhash2row, contexthash2col, wh_ch_pairs)\n",
    "        self.word_vectors = self.svd(M)\n",
    "        return self\n",
    "    \n",
    "    def get_word_vectors(self):\n",
    "        return pd.DataFrame(data = self.word_vectors, index = self.words, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MFWordEmbedder at 0x7fbbd0098650>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MFWordEmbedder(min_wf=2, min_cf=1, window=2, vec_dim=2)\n",
    "a = corpus[:10] * 2\n",
    "print a\n",
    "model.fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 23.2 s, total: 2min 25s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MFWordEmbedder at 0x7fbbb3fc9210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = MFWordEmbedder(svdtype=\"truncated\", window=2, min_cf=4, vec_dim=100)\n",
    "model = MFWordEmbedder(svdtype=\"symmetric\", window=2, min_cf=3, vec_dim=100)\n",
    "%time model.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 ms, sys: 389 µs, total: 1.48 ms\n",
      "Wall time: 1.42 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>king</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081232</td>\n",
       "      <td>-0.010924</td>\n",
       "      <td>-0.151333</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>-0.172565</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>-0.054948</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>0.078047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queen</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018251</td>\n",
       "      <td>-0.012156</td>\n",
       "      <td>-0.058302</td>\n",
       "      <td>0.027453</td>\n",
       "      <td>-0.037100</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>-0.012540</td>\n",
       "      <td>-0.000916</td>\n",
       "      <td>0.015875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "king   0.000052  0.000788  0.001362  0.001186  0.000378  0.002197  0.001757   \n",
       "queen  0.000021  0.000160  0.000357  0.000888  0.000292  0.000574  0.000179   \n",
       "\n",
       "             7         8         9     ...           90        91        92  \\\n",
       "king   0.008872  0.011870 -0.000413    ...    -0.081232 -0.010924 -0.151333   \n",
       "queen  0.004333  0.005334 -0.000206    ...    -0.018251 -0.012156 -0.058302   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "king   0.067235 -0.172565  0.067491  0.021508 -0.054948  0.047741  0.078047  \n",
       "queen  0.027453 -0.037100  0.023543  0.002896 -0.012540 -0.000916  0.015875  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time wvs = model.get_word_vectors()\n",
    "wvs.loc[[\"king\", \"queen\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 10401)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(wvs.sum(axis = 1)==0), wvs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use flann to find knn\n",
    "import pyflann as pf\n",
    "from scipy import stats\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self, k = 5, algorithm=\"kdtree\", distance_type=\"euclidean\"):\n",
    "        pf.set_distance_type(distance_type)\n",
    "        self.flann = pf.FLANN()\n",
    "        self.k = k\n",
    "        self.algorithm = \"autotuned\"#algorithm\n",
    "        self.iterations = 100\n",
    "    def train(self, X):\n",
    "        self.X_ = X\n",
    "    def nearest(self, X):\n",
    "        min_index, dists = self.flann.nn(self.X_, X, self.k, \n",
    "                                         algorithm = self.algorithm, \n",
    "                                         iterations=self.iterations)\n",
    "        return min_index, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 1.99 s, total: 1min 15s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "nn=NearestNeighbor()\n",
    "nn.train(model.word_vectors)\n",
    "%time neighbors, dists =nn.nearest(model.word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['east', 'west', 'south', 'north', 'central']\n"
     ]
    }
   ],
   "source": [
    "for row in neighbors:\n",
    "    close_words = [model.words[r] for r in row]\n",
    "    if close_words[0] == \"east\":\n",
    "        print close_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
