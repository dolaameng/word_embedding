{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Word Vectors\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)\n",
    "- [text8 data](http://mattmahoney.net/dc/text8.zip)\n",
    "- [wordsim353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
    "- [MEN Test collection](http://clic.cimec.unitn.it/~elia.bruni/MEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change from v2 to v3: overwrite sklearn.TruncatedSVD to make it symmetric as suggested in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters\n",
    "## observations WC_THR to be lower, CC_THR to be > 4\n",
    "WC_THR = 100\n",
    "CC_THR = 6\n",
    "window = 2\n",
    "nnegative = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 s, sys: 422 ms, total: 3.5 s\n",
      "Wall time: 3.51 s\n",
      "17005207\n"
     ]
    }
   ],
   "source": [
    "corpus = open(\"data/text8\").read()\n",
    "pat = re.compile(\"\\w+\")\n",
    "%time words = pat.findall(corpus)\n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.71 s, sys: 29.3 ms, total: 4.74 s\n",
      "Wall time: 4.71 s\n",
      "0.0465425008076 % words are retained\n",
      "CPU times: user 25.6 s, sys: 747 ms, total: 26.3 s\n",
      "Wall time: 26.3 s\n",
      "0.00079570578834 % contexts are retained\n"
     ]
    }
   ],
   "source": [
    "%time total_word_counter = Counter(words)\n",
    "\n",
    "freq_words = set([w for w,c in total_word_counter.items() if c >= WC_THR])\n",
    "print len(freq_words) *1. / len(total_word_counter), \"% words are retained\"\n",
    "\n",
    "\n",
    "%time total_context_counter = Counter([tuple(words[i-window:i]+words[i+1:i+window+1]) for i in xrange(window, len(words)-window)])\n",
    "freq_contexts = set([w for w,c in total_context_counter.items() if c >= CC_THR])\n",
    "print len(freq_words) *1. / len(total_context_counter), \"% contexts are retained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hasher(object):\n",
    "    def __init__(self):\n",
    "        self.N = 0\n",
    "        self.data = {}\n",
    "    def hash(self,item):\n",
    "        if item not in self.data:\n",
    "            self.data[item] = self.N\n",
    "            self.N += 1\n",
    "        return self.data[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_wc_pairs(words,  freq_words, freq_contexts, window = 3,):\n",
    "    wcpairs = []\n",
    "    wordhasher = Hasher()\n",
    "    contexthasher = Hasher()\n",
    "    for i in xrange(window, len(words)-window):\n",
    "        word, context = words[i], tuple(words[i-window:i]+words[i+1:i+window+1])\n",
    "        if (word in freq_words) and (context in freq_contexts):\n",
    "            hw, hc = wordhasher.hash(word), contexthasher.hash(context)\n",
    "            wcpairs.append( (hw, hc) )\n",
    "    return wordhasher, contexthasher, wcpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 83.3 ms, total: 12.5 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%time wordhasher, contexthasher, wcpairs = extract_wc_pairs(words, freq_words, freq_contexts, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 388 ms, sys: 5.52 ms, total: 393 ms\n",
      "Wall time: 394 ms\n"
     ]
    }
   ],
   "source": [
    "%time whs, chs = zip(*wcpairs)\n",
    "whs, chs = np.asarray(whs), np.asarray(chs)\n",
    "Mshape = (wordhasher.N, contexthasher.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 483 ms, sys: 23.8 ms, total: 507 ms\n",
      "Wall time: 458 ms\n",
      "CPU times: user 482 ms, sys: 9.95 ms, total: 492 ms\n",
      "Wall time: 476 ms\n",
      "CPU times: user 1.11 s, sys: 2.46 ms, total: 1.11 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%time wcounter = Counter(whs)\n",
    "%time ccounter = Counter(chs)\n",
    "D = len(wcpairs)\n",
    "%time data = np.array([D*1./wcounter[wh]/ccounter[ch]/nnegative for wh,ch in wcpairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 249 µs, total: 10.3 ms\n",
      "Wall time: 10 ms\n",
      "CPU times: user 114 ms, sys: 388 µs, total: 114 ms\n",
      "Wall time: 114 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10796, 68388)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time M = sparse.coo_matrix((data, (whs, chs)), shape = Mshape, dtype=np.float32)\n",
    "%time M = M.tocsr()\n",
    "M.data = np.log(M.data)##\n",
    "M[M<0.0] = 0.0\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils import check_array, as_float_array, check_random_state\n",
    "from sklearn.utils.extmath import randomized_svd, safe_sparse_dot, svd_flip\n",
    "from sklearn.utils.sparsefuncs import mean_variance_axis\n",
    "\n",
    "import scipy.sparse as sp\n",
    "class SymmetricSVD(TruncatedSVD):\n",
    "    def __init__(self, n_components=2, algorithm=\"randomized\", n_iter=5,\n",
    "                    random_state=None, tol=0.):\n",
    "        super(SymmetricSVD, self).__init__(n_components, algorithm, \n",
    "                                          n_iter, random_state, tol)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        After svd, we have M = U * Sigma * VT, traditional SVD\n",
    "        return W = U * Sigma as the transformed vectors; here \n",
    "        in SymmetricSVD version, it is W = U * sqrt(Sigma) that is returned\n",
    "        as the transformed vectors.\n",
    "        In the paper [Neural Word Embedding as Implicit Matrix Factorization], the \n",
    "        authors claim that it is not clear why this works better, but it does work\n",
    "        better than the traditional approach.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Reduced version of X. This will always be a dense array.\n",
    "        \"\"\"\n",
    "        X = as_float_array(X, copy=False)\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        # If sparse and not csr or csc, convert to csr\n",
    "        if sp.issparse(X) and X.getformat() not in [\"csr\", \"csc\"]:\n",
    "            X = X.tocsr()\n",
    "\n",
    "        if self.algorithm == \"arpack\":\n",
    "            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "            # conventions, so reverse its outputs.\n",
    "            Sigma = Sigma[::-1]\n",
    "            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "        elif self.algorithm == \"randomized\":\n",
    "            k = self.n_components\n",
    "            n_features = X.shape[1]\n",
    "            if k >= n_features:\n",
    "                raise ValueError(\"n_components must be < n_features;\"\n",
    "                                 \" got %d >= %d\" % (k, n_features))\n",
    "            U, Sigma, VT = randomized_svd(X, self.n_components,\n",
    "                                          n_iter=self.n_iter,\n",
    "                                          random_state=random_state)\n",
    "        else:\n",
    "            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n",
    "\n",
    "        self.components_ = VT\n",
    "\n",
    "        # Calculate explained variance & explained variance ratio\n",
    "        ## USE SQRT OF SIGMA INSTEAD OF SIGMA ITSELF\n",
    "        X_transformed = np.dot(U, np.sqrt(np.diag(Sigma)))\n",
    "        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n",
    "        if sp.issparse(X):\n",
    "            _, full_var = mean_variance_axis(X, axis=0)\n",
    "            full_var = full_var.sum()\n",
    "        else:\n",
    "            full_var = np.var(X, axis=0).sum()\n",
    "        self.explained_variance_ratio_ = exp_var / full_var\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#svd = TruncatedSVD(n_components=100, )\n",
    "svd = SymmetricSVD(n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 7.01 s, total: 22.2 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%time wvectors = svd.fit_transform(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normalized_wvectors = normalize(wvectors, axis = 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-12.61808452285581,\n",
       " 11.306756175983681,\n",
       " -0.91601676607624627,\n",
       " 0.89861550816679781)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wvectors.min(), wvectors.max(), normalized_wvectors.min(), normalized_wvectors.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check word-context matrix & word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEACAYAAACXqUyYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdaf1922850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.boxplot(M.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 ms, sys: 6.8 ms, total: 18.6 ms\n",
      "Wall time: 6.07 ms\n",
      "CPU times: user 524 ms, sys: 143 ms, total: 667 ms\n",
      "Wall time: 561 ms\n"
     ]
    }
   ],
   "source": [
    "## inverse index\n",
    "%time h2w = dict([(v,k) for k,v in wordhasher.data.items()])\n",
    "%time h2c = dict([(v,k) for k,v in contexthasher.data.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 413 µs, sys: 5.25 ms, total: 5.66 ms\n",
      "Wall time: 5.68 ms\n",
      "CPU times: user 13.4 ms, sys: 597 µs, total: 14 ms\n",
      "Wall time: 13.8 ms\n",
      "CPU times: user 10.7 ms, sys: 448 µs, total: 11.2 ms\n",
      "Wall time: 11.2 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import h5py, cPickle\n",
    "\n",
    "h5f = h5py.File(\"data/word_vectors.h5\", \"w\")\n",
    "%time h5f.create_dataset(\"data\", data = wvectors)\n",
    "h5f.close()\n",
    "\n",
    "%time cPickle.dump(h2w, open(\"data/inverse_word_hash.pkl\", \"w\"))\n",
    "%time cPickle.dump(wordhasher.data, open(\"data/word_hash.pkl\", \"w\"))\n",
    "#%time cPickle.dump(h2c, open(\"data/inverse_context_hash.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py, cPickle\n",
    "h5f = h5py.File(\"data/word_vectors.h5\", \"r\")\n",
    "wvectors = h5f[\"data\"][:]\n",
    "h5f.close()\n",
    "\n",
    "h2w = cPickle.load(open(\"data/inverse_word_hash.pkl\"))\n",
    "w2h = cPickle.load(open(\"data/word_hash.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use flann to find knn\n",
    "import pyflann as pf\n",
    "from scipy import stats\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self, k = 5, algorithm=\"kdtree\", distance_type=\"euclidean\"):\n",
    "        pf.set_distance_type(distance_type)\n",
    "        self.flann = pf.FLANN()\n",
    "        self.k = k\n",
    "        self.algorithm = \"autotuned\"#algorithm\n",
    "        self.iterations = 100\n",
    "    def train(self, X):\n",
    "        self.X_ = X\n",
    "    def nearest(self, X):\n",
    "        min_index, dists = self.flann.nn(self.X_, X, self.k, \n",
    "                                         algorithm = self.algorithm, \n",
    "                                         iterations=self.iterations)\n",
    "        return min_index, dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 1.95 s, total: 1min 23s\n",
      "Wall time: 1min 23s\n",
      "['destroyed', 'banned', 'commissioned', 'rejected', 'formed']\n",
      "['danger', 'gender', 'backing', 'constraints', 'placing']\n",
      "['cyclic', 'respective', 'residential', 'counterpoint', 'excessive']\n",
      "['consequence', 'measure', 'descendant', 'continuation', 'friend']\n",
      "['addressed', 'explored', 'borrowed', 'sung', 'retired']\n",
      "['trend', 'meditation', 'think', 'fortress', 'understand']\n",
      "['canadian', 'australian', 'danish', 'austrian', 'mexican']\n",
      "['archaeological', 'slight', 'insufficient', 'genetic', 'how']\n",
      "['place', 'position', 'level', 'space', 'role']\n",
      "['even', 'quite', 'perhaps', 'actually', 'therefore']\n",
      "['alchemy', 'rises', 'bilateral', 'cobalt', 'skepticism']\n",
      "['islamic', 'communist', 'free', 'democratic', 'medieval']\n",
      "['results', 'consequences', 'positions', 'behavior', 'limitations']\n",
      "['goals', 'causes', 'roles', 'positions', 'accuracy']\n",
      "['search', 'flames', 'contest', 'talent', 'cavity']\n",
      "['industries', 'quantities', 'mercedes', 'luxury', 'networks']\n",
      "['craft', 'expressions', 'atlas', 'siblings', 'get']\n",
      "['corruption', 'projection', 'measurement', 'dialect', 'breach']\n",
      "['material', 'contract', 'tree', 'dialogue', 'medium']\n",
      "['evolution', 'authorship', 'conduct', 'production', 'sinking']\n",
      "['separate', 'distinct', 'wider', 'legal', 'logical']\n",
      "['birth', 'murder', 'reign', 'execution', 'appointment']\n",
      "['sun', 'moon', 'pope', 'prc', 'ball']\n",
      "['occasionally', 'popularly', 'normally', 'traditionally', 'being']\n",
      "['writings', 'studies', 'actions', 'minds', 'affairs']\n",
      "['suggested', 'argued', 'stated', 'speculated', 'alleged']\n",
      "['coptic', 'lithuanian', 'manx', 'pakistani', 'galactic']\n",
      "['ancient', 'arab', 'old', 'entire', 'former']\n",
      "['domain', 'basin', 'tale', 'cardinality', 'framework']\n",
      "['speculated', 'hoped', 'agreed', 'predicted', 'here']\n",
      "['carried', 'transmitted', 'drawn', 'divided', 'detected']\n",
      "['compared', 'identical', 'tied', 'returned', 'analogous']\n",
      "['mongols', 'vikings', 'bengals', 'bolsheviks', 'elector']\n",
      "['medicine', 'micronesia', 'banking', 'anarchism', 'karate']\n",
      "['west', 'east', 'south', 'southwest', 'americas']\n",
      "['metaphysics', 'quotations', 'designations', 'packages', 'multimedia']\n",
      "['belief', 'fact', 'possibility', 'claim', 'extent']\n",
      "['determined', 'represented', 'provided', 'controlled', 'caused']\n",
      "['christian', 'muslim', 'islamic', 'conservative', 'democratic']\n",
      "['flawed', 'yin', 'measurable', 'touring', 'searching']\n",
      "['classic', 'contemporary', 'literary', 'graduate', 'pejorative']\n",
      "['creatures', 'spear', 'twist', 'hull', 'spice']\n",
      "['elements', 'components', 'features', 'aspects', 'branches']\n",
      "['joined', 'directed', 'administered', 'passed', 'driven']\n",
      "['egyptians', 'greens', 'exotic', 'dolphins', 'vectors']\n",
      "['christianity', 'eastenders', 'macedonia', 'arthur', 'you']\n",
      "['fall', 'defeat', 'dissolution', 'demise', 'advent']\n",
      "['arabic', 'hebrew', 'latin', 'industrial', 'indonesian']\n",
      "['stage', 'step', 'leg', 'amendment', 'moment']\n",
      "['pioneer', 'favorite', 'manifestation', 'mascot', 'protagonist']\n",
      "['deal', 'hotel', 'bombs', 'demographic', 'dam']\n",
      "['ways', 'circumstances', 'applications', 'jurisdictions', 'contexts']\n",
      "['peace', 'photons', 'witches', 'medici', 'mosque']\n",
      "['opinion', 'premise', 'impression', 'interest', 'dna']\n",
      "['sanctioned', 'multiplied', 'accumulated', 'possessed', 'exemplified']\n",
      "['church', 'senate', 'community', 'parliament', 'dead']\n",
      "['tomb', 'basin', 'chapel', 'shrine', 'revolt']\n",
      "['predecessors', 'sons', 'travels', 'generals', 'ride']\n",
      "['art', 'plot', 'sect', 'hill', 'charter']\n",
      "['conflict', 'macintosh', 'atom', 'league', 'lake']\n",
      "['himself', 'themselves', 'circumcision', 'you', 'einstein']\n",
      "['harmony', 'ensemble', 'accordance', 'praise', 'nehru']\n",
      "['image', 'interpretation', 'pronunciation', 'bed', 'report']\n",
      "['seeds', 'lawyers', 'addresses', 'exit', 'corps']\n",
      "['chemistry', 'biology', 'psychology', 'forums', 'recommendations']\n",
      "['claims', 'devices', 'machines', 'operas', 'ingredients']\n",
      "['idea', 'concept', 'notion', 'possibility', 'extent']\n",
      "['cloning', 'positioning', 'valves', 'hunters', 'differential']\n",
      "['portion', 'section', 'branch', 'component', 'division']\n",
      "['range', 'amount', 'capacity', 'height', 'ratio']\n",
      "['nautical', 'explain', 'aq', 'thermal', 'introducing']\n",
      "['vicinity', 'aftermath', 'wake', 'shadow', 'manufacture']\n",
      "['aircraft', 'gene', 'computers', 'rocks', 'similarities']\n",
      "['sole', 'ultimate', 'eternal', 'excess', 'canonical']\n",
      "['sent', 'brought', 'transferred', 'attributed', 'closer']\n",
      "['variety', 'collection', 'minority', 'mixture', 'lot']\n",
      "['discussed', 'repeated', 'manufactured', 'retained', 'grown']\n",
      "['susceptible', 'donated', 'wired', 'ascribed', 'damaging']\n",
      "['mhz', 'nm', 'millimeters', 'hz', 'pm']\n",
      "['upper', 'northeastern', 'southwestern', 'lower', 'northwestern']\n",
      "['landlocked', 'touring', 'associative', 'irreducible', 'krishna']\n",
      "['nations', 'kings', 'animals', 'territories', 'towers']\n",
      "['seat', 'headquarters', 'birthplace', 'focus', 'ancestor']\n",
      "['presidency', 'rebels', 'commission', 'nobility', 'cup']\n",
      "['derived', 'obtained', 'removed', 'combined', 'accomplished']\n",
      "['understood', 'classified', 'treated', 'expressed', 'solved']\n",
      "['situated', 'engaged', 'referenced', 'displayed', 'reflected']\n",
      "['eastern', 'northern', 'southern', 'western', 'southeastern']\n",
      "['edge', 'coast', 'tip', 'bottom', 'shore']\n",
      "['holy', 'inner', 'scandinavian', 'rural', 'external']\n",
      "['conquered', 'rejected', 'superseded', 'purchased', 'occupied']\n",
      "['romans', 'pope', 'portuguese', 'allies', 'nazis']\n",
      "['vienna', 'rome', 'geneva', 'edinburgh', 'berlin']\n",
      "['line', 'circle', 'verb', 'noun', 'molecule']\n",
      "['short', 'long', 'brief', 'minimal', 'longer']\n",
      "['austria', 'denmark', 'norway', 'castile', 'spain']\n",
      "['until', 'since', 'through', 'gwh', 'inducted']\n",
      "['habsburg', 'flanders', 'zealand', 'burma', 'napoleonic']\n",
      "['abolition', 'invention', 'dissolution', 'passing', 'extinction']\n",
      "['empire', 'crown', 'navy', 'republic', 'mission']\n"
     ]
    }
   ],
   "source": [
    "nn=NearestNeighbor()\n",
    "nn.train(wvectors)\n",
    "%time neighbors, dists =nn.nearest(wvectors)\n",
    "for row in neighbors[1000:1100, :]:\n",
    "    print [h2w[r] for r in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 2.48 s, total: 1min 23s\n",
      "Wall time: 1min 22s\n",
      "['destroyed', 'abandoned', 'signed', 'banned', 'diminished']\n",
      "['danger', 'likelihood', 'bias', 'pitch', 'structure']\n",
      "['hungry', 'm', 'iec', 'loses', 'maize']\n",
      "['consequence', 'gentleman', 'feed', 'favourite', 'mating']\n",
      "['discussed', 'used', 'inclined', 'witnessed', 'adam']\n",
      "['trend', 'break', 'port', 'rebel', 'fortress']\n",
      "['canadian', 'libyan', 'cuban', 'egyptian', 'danish']\n",
      "['quarterly', 'international', 'maritime', 'ecumenical', 'national']\n",
      "['place', 'dinner', 'role', 'position', 'ambient']\n",
      "['even', 'barely', 'perfectly', 'also', 'theoretically']\n",
      "['imperialism', 'telecommunications', 'hungry', 'mc', 'prix']\n",
      "['islamic', 'modern', 'feudal', 'christian', 'kurdish']\n",
      "['results', 'fate', 'independence', 'truth', 'feeling']\n",
      "['goals', 'causes', 'objectives', 'roles', 'importance']\n",
      "['search', 'hunt', 'ratings', 'campaign', 'competition']\n",
      "['telecommunications', 'draws', 'peers', 'expressions', 'conversation']\n",
      "['expressions', 'peers', 'avenue', 'philosophy', 'except']\n",
      "['corruption', 'projection', 'privilege', 'manifestation', 'ray']\n",
      "['material', 'verb', 'contract', 'deceased', 'peroxide']\n",
      "['evolution', 'making', 'majesty', 'initiation', 'ancestry']\n",
      "['separate', 'distinct', 'racial', 'new', 'modern']\n",
      "['birth', 'death', 'assassination', 'unemployment', 'arrest']\n",
      "['sun', 'heinz', 'papacy', 'os', 'devil']\n",
      "['occasionally', 'commonly', 'widely', 'thereafter', 'frequently']\n",
      "['writings', 'works', 'woodland', 'honor', 'life']\n",
      "['suggested', 'speculated', 'argued', 'feared', 'believes']\n",
      "['individualist', 'entering', 'respect', 'emerged', 'pngimage']\n",
      "['ancient', 'arab', 'terrestrial', 'developing', 'old']\n",
      "['domain', 'core', 'limits', 'geometry', 'shape']\n",
      "['speculated', 'argued', 'feared', 'suggested', 'believes']\n",
      "['carried', 'fired', 'drawn', 'expanded', 'improved']\n",
      "['compared', 'connected', 'linked', 'identical', 'wired']\n",
      "['mongols', 'vikings', 'crusaders', 'nazis', 'turks']\n",
      "['terror', 'your', 'implies', 'telecommunications', 'fansite']\n",
      "['west', 'east', 'silk', 'lockheed', 'southeast']\n",
      "['peers', 'expressions', 'hungry', 'telecommunications', 'emerged']\n",
      "['belief', 'fact', 'hypothesis', 'conjecture', 'notions']\n",
      "['determined', 'indicated', 'provided', 'explained', 'evaluated']\n",
      "['christian', 'muslim', 'islamic', 'bah', 'thermodynamics']\n",
      "['emerged', 'conspiracy', 'astros', 'clocks', 'pr']\n",
      "['tribal', 'international', 'maritime', 'planetary', 'ecumenical']\n",
      "['input', 'aspartame', 'seizures', 'lung', 'juice']\n",
      "['elements', 'features', 'components', 'kinds', 'luxury']\n",
      "['joined', 'directed', 'supplied', 'passed', 'administered']\n",
      "['abbreviated', 'qualification', 'emigration', 'fbi', 'saxons']\n",
      "['christianity', 'mtv', 'augustus', 'autism', 'brown']\n",
      "['fall', 'demise', 'defeat', 'dissolution', 'destruction']\n",
      "['arabic', 'cyrillic', 'disney', 'hebrew', 'latin']\n",
      "['stage', 'digit', 'step', 'card', 'pick']\n",
      "['pioneer', 'patron', 'masterpiece', 'favorite', 'merge']\n",
      "['deal', 'cope', 'stand', 'bang', 'satellites']\n",
      "['ways', 'environments', 'situations', 'markets', 'contexts']\n",
      "['aspartame', 'dead', 'mahayana', 'patents', 'imperfect']\n",
      "['opinion', 'probability', 'argument', 'risk', 'biography']\n",
      "['clocks', 'transfers', 'astros', 'pr', 'except']\n",
      "['church', 'unesco', 'disciples', 'empire', 'monastery']\n",
      "['tomb', 'valley', 'affairs', 'adventure', 'competition']\n",
      "['peers', 'fathers', 'doom', 'grandmother', 'spirits']\n",
      "['philosophy', 'technology', 'garbage', 'science', 'interrupt']\n",
      "['conflict', 'dispute', 'taxi', 'revolution', 'fight']\n",
      "['himself', 'him', 'themselves', 'them', 'ale']\n",
      "['metadata', 'praise', 'goodness', 'orioles', 'astros']\n",
      "['image', 'action', 'account', 'interpretation', 'understanding']\n",
      "['goal', 'securing', 'destroyer', 'lasts', 'prophecy']\n",
      "['chemistry', 'anarchism', 'crick', 'goodness', 'flourished']\n",
      "['peers', 'expressions', 'tablets', 'tests', 'goodness']\n",
      "['idea', 'notion', 'idealism', 'sentiment', 'fact']\n",
      "['switching', 'horns', 'grant', 'purchasing', 'clocks']\n",
      "['portion', 'stretch', 'section', 'infringement', 'percentage']\n",
      "['range', 'longitude', 'amount', 'scattering', 'yield']\n",
      "['households', 'telecommunications', 'mc', 'saga', 'plutonium']\n",
      "['vicinity', 'employ', 'midst', 'wake', 'eyes']\n",
      "['aircraft', 'spacecraft', 'computer', 'virus', 'particle']\n",
      "['sole', 'emerged', 'twice', 'again', 'draws']\n",
      "['sent', 'deported', 'posted', 'transferred', 'assigned']\n",
      "['variety', 'multitude', 'number', 'handful', 'occupy']\n",
      "['discussed', 'witnessed', 'used', 'corrupt', 'adam']\n",
      "['lent', 'subordinate', 'referring', 'accessible', 'alien']\n",
      "['mhz', 'cc', 'jason', 'tons', 'trucks']\n",
      "['upper', 'eastern', 'outer', 'northeastern', 'lower']\n",
      "['congressman', 'hungry', 'prix', 'mint', 'army']\n",
      "['nations', 'states', 'kingdom', 'lyons', 'firms']\n",
      "['seat', 'flagship', 'birthplace', 'headquarters', 'residence']\n",
      "['presidency', 'regime', 'rebels', 'imf', 'devil']\n",
      "['derived', 'extracted', 'descended', 'obtained', 'pulled']\n",
      "['understood', 'defined', 'imagined', 'perceived', 'labelled']\n",
      "['situated', 'located', 'placed', 'burnt', 'bleeding']\n",
      "['eastern', 'southeastern', 'southern', 'southwestern', 'northern']\n",
      "['edge', 'tip', 'shores', 'shore', 'side']\n",
      "['holy', 'upper', 'lower', 'dry', 'mining']\n",
      "['conquered', 'purchased', 'rejected', 'superseded', 'halted']\n",
      "['romans', 'pope', 'cia', 'persians', 'nazis']\n",
      "['vienna', 'oslo', 'geneva', 'moscow', 'paris']\n",
      "['line', 'code', 'circle', 'horse', 'noun']\n",
      "['short', 'brief', 'long', 'lengthy', 'associative']\n",
      "['austria', 'denmark', 'poland', 'russia', 'portugal']\n",
      "['until', 'since', 'through', 'before', 'throughout']\n",
      "['stanley', 'captivity', 'operative', 'chord', 'talks']\n",
      "['abolition', 'invention', 'destruction', 'dissolution', 'passing']\n",
      "['empire', 'admiralty', 'lynx', 'ppp', 'senate']\n"
     ]
    }
   ],
   "source": [
    "norm_nn=NearestNeighbor()\n",
    "norm_nn.train(normalized_wvectors)\n",
    "%time norm_neighbors, norm_dists = norm_nn.nearest(wvectors)\n",
    "for row in norm_neighbors[1000:1100, :]:\n",
    "    print [h2w[r] for r in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-12.61808452285581,\n",
       " 11.306756175983681,\n",
       " -0.91601676607624627,\n",
       " 0.89861550816679781)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wvectors.min(), wvectors.max(), normalized_wvectors.min(), normalized_wvectors.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 189.77789161933046, 1.9975954652178631e-08, 164.65868506956568)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.min(), dists.max(), norm_dists.min(), norm_dists.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.09 s, sys: 6.18 s, total: 12.3 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=250, )\n",
    "%time labels = kmeans.fit_predict(normalized_wvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({187: 181, 220: 177, 65: 167, 93: 151, 71: 148, 14: 139, 236: 137, 3: 135, 124: 133, 114: 130, 219: 126, 203: 116, 59: 111, 143: 107, 40: 106, 46: 105, 116: 105, 52: 97, 104: 89, 148: 89, 118: 86, 41: 85, 210: 82, 13: 81, 240: 81, 44: 80, 217: 80, 246: 80, 247: 80, 10: 79, 131: 79, 163: 79, 16: 78, 28: 78, 54: 78, 115: 77, 35: 75, 170: 73, 235: 73, 151: 72, 4: 71, 68: 69, 109: 69, 80: 68, 201: 68, 17: 66, 171: 65, 75: 64, 101: 64, 149: 64, 202: 64, 21: 63, 106: 63, 132: 63, 142: 63, 226: 63, 67: 62, 156: 62, 208: 62, 48: 61, 61: 61, 123: 61, 233: 59, 184: 58, 216: 58, 5: 57, 60: 57, 87: 56, 110: 56, 185: 56, 160: 55, 162: 55, 73: 54, 113: 54, 165: 54, 231: 54, 6: 53, 23: 53, 96: 53, 215: 52, 31: 51, 150: 51, 158: 51, 166: 51, 83: 50, 97: 50, 138: 50, 178: 50, 221: 50, 225: 50, 70: 49, 130: 49, 179: 49, 238: 49, 20: 48, 69: 48, 128: 47, 204: 47, 29: 46, 57: 46, 117: 46, 43: 45, 37: 44, 89: 44, 8: 43, 32: 43, 78: 43, 103: 43, 153: 43, 194: 43, 211: 43, 76: 42, 141: 42, 180: 42, 186: 42, 243: 42, 53: 41, 133: 41, 155: 41, 84: 40, 134: 39, 232: 39, 33: 38, 173: 38, 197: 38, 22: 37, 66: 37, 218: 37, 1: 36, 45: 36, 62: 36, 242: 36, 47: 35, 58: 35, 161: 35, 214: 35, 18: 34, 209: 34, 82: 33, 147: 33, 157: 33, 177: 33, 199: 33, 229: 33, 230: 33, 192: 32, 237: 32, 38: 31, 56: 31, 91: 31, 213: 31, 105: 30, 169: 30, 200: 30, 0: 29, 193: 29, 196: 29, 241: 29, 77: 28, 102: 28, 108: 28, 27: 27, 122: 27, 145: 27, 137: 26, 90: 25, 42: 24, 85: 24, 94: 24, 121: 24, 88: 23, 146: 23, 175: 23, 25: 21, 126: 21, 11: 20, 26: 20, 119: 20, 167: 19, 168: 19, 224: 19, 2: 18, 86: 18, 107: 18, 207: 18, 50: 17, 174: 17, 188: 17, 206: 17, 159: 16, 81: 15, 111: 15, 164: 15, 195: 15, 198: 15, 12: 14, 36: 14, 144: 14, 182: 14, 234: 14, 249: 14, 49: 13, 95: 13, 190: 13, 34: 12, 245: 12, 125: 11, 176: 11, 222: 11, 63: 10, 183: 10, 79: 9, 127: 9, 139: 9, 24: 8, 92: 8, 129: 8, 135: 8, 248: 8, 154: 7, 7: 6, 30: 6, 98: 6, 140: 6, 172: 6, 189: 6, 74: 5, 191: 5, 212: 5, 223: 5, 228: 5, 205: 4, 227: 4, 9: 3, 39: 3, 72: 3, 100: 3, 51: 2, 55: 2, 64: 2, 99: 2, 112: 2, 181: 2, 244: 2, 15: 1, 19: 1, 120: 1, 136: 1, 152: 1, 239: 1})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure\n",
      "role\n",
      "increase\n",
      "character\n",
      "rebel\n",
      "step\n",
      "walk\n",
      "surplus\n",
      "trend\n",
      "place\n",
      "break\n",
      "note\n",
      "refuge\n",
      "element\n",
      "difference\n",
      "layer\n",
      "participation\n",
      "ida\n",
      "change\n",
      "advantage\n",
      "draw\n",
      "port\n",
      "factor\n",
      "advance\n",
      "deficit\n",
      "decrease\n",
      "ambient\n",
      "hollow\n",
      "hole\n",
      "intermediate\n",
      "participant\n",
      "unit\n",
      "curve\n",
      "prevent\n",
      "stop\n",
      "reduction\n",
      "reputation\n",
      "clause\n",
      "leaf\n",
      "stake\n",
      "expert\n",
      "digimon\n",
      "drop\n",
      "bugs\n",
      "multiply\n",
      "divide\n",
      "certainty\n",
      "fault\n",
      "shift\n",
      "festival\n"
     ]
    }
   ],
   "source": [
    "ic = 217\n",
    "for i in np.where(labels==ic)[0][:50]:\n",
    "    print h2w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ifrance, iparis, iuk, ilondon = w2h[\"king\"], w2h[\"man\"], w2h[\"queen\"], w2h[\"woman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = wvectors[ifrance] - wvectors[iparis] + wvectors[iuk]\n",
    "u = wvectors[ilondon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = normalized_wvectors[ifrance] - normalized_wvectors[iparis] + normalized_wvectors[iuk]\n",
    "u = normalized_wvectors[ilondon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3329281354673705"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum((u-v)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  1.17757446,  2.58352333,  3.09904618,  3.1063632 ])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dists[ilondon, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'player', 'bishop', 'plant', 'lion']\n"
     ]
    }
   ],
   "source": [
    "for row in neighbors[ilondon:ilondon+1, :]:\n",
    "    print [h2w[r] for r in row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Observations\n",
    "1. for the matrix factorization approach to work, filtering out low-freq words (and even contexts) is important, as otherwise the factorization will come up with a near-zero solution\n",
    "2. it seems that the non-symmetric svd gives better results in terms of vector algebra..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
